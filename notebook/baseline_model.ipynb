{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c77973",
   "metadata": {},
   "source": [
    "# Clean Baseline Curve Fitting for Air Filter Restriction\n",
    "\n",
    "## Objective\n",
    "Fit a clean baseline curve **R_clean(HP)** for each asset using:\n",
    "- **Isotonic Regression** for monotonic relationship\n",
    "- **Linear extrapolation** to cover full operating range\n",
    "- **5th percentile binning** to capture lower envelope (clean filter behavior)\n",
    "\n",
    "## Key Challenge\n",
    "We need to predict air filter restriction across the **entire HP range**, but clean baseline data only exists in lower restriction ranges. Solution: forced extrapolation to asset limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = \"../data/air_filter_data.csv\"\n",
    "LIMITS_PATH = \"../data/asset_limits.csv\"\n",
    "\n",
    "# Processing parameters\n",
    "CHUNKSIZE = 100_000      # For memory-efficient loading\n",
    "HP_BIN_WIDTH = 50        # HP bin width for aggregation\n",
    "PERCENTILE = 5           # Lower envelope (5th percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f406f",
   "metadata": {},
   "source": [
    "## Step 1: Load Asset Operating Limits\n",
    "Each asset has maximum HP and restriction values from engineering specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacd815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load asset limits\n",
    "limits_df = pd.read_csv(LIMITS_PATH)\n",
    "\n",
    "def normalize_asset_id(val):\n",
    "    \"\"\"Handles both float and string asset IDs\"\"\"\n",
    "    try:\n",
    "        return str(int(float(val)))\n",
    "    except Exception:\n",
    "        return str(val)\n",
    "\n",
    "asset_limits = {\n",
    "    normalize_asset_id(row['asset']): {\n",
    "        'max_restriction': row['Max_AirFilterRestriction'],\n",
    "        'max_hp': row['Max_Horsepower']\n",
    "    }\n",
    "    for _, row in limits_df.iterrows()\n",
    "}\n",
    "\n",
    "print(f\"Loaded limits for {len(asset_limits)} assets\")\n",
    "print(f\"Example: Asset limits = {list(asset_limits.items())[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96c9e6",
   "metadata": {},
   "source": [
    "## Step 2: Find Assets in Both Datasets\n",
    "Only process assets present in both air_filter_data.csv and asset_limits.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scanning available assets...\")\n",
    "asset_ids = set()\n",
    "all_data_asset_ids = set()\n",
    "\n",
    "for chunk in pd.read_csv(DATA_PATH, usecols=[\"asset\"], chunksize=CHUNKSIZE):\n",
    "    chunk_ids = set(normalize_asset_id(a) for a in chunk[\"asset\"].unique())\n",
    "    all_data_asset_ids.update(chunk_ids)\n",
    "    asset_ids.update(chunk_ids)\n",
    "\n",
    "print(f\"Assets in air_filter_data.csv: {sorted(all_data_asset_ids)}\")\n",
    "print(f\"Assets in asset_limits.csv: {sorted(asset_limits.keys())}\")\n",
    "\n",
    "asset_ids = sorted([aid for aid in asset_ids if aid in asset_limits])\n",
    "print(f\"\\n✓ Processing {len(asset_ids)} assets: {asset_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13485a",
   "metadata": {},
   "source": [
    "## Step 3: Process Each Asset\n",
    "\n",
    "### 3.1 Load Asset Data\n",
    "Load HP and Restriction data for one asset (memory-efficient chunked reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da491b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one asset for demonstration\n",
    "asset_id = asset_ids[0]  # Change index to view different assets\n",
    "print(f\"Processing asset {asset_id}\")\n",
    "\n",
    "# Create output directory\n",
    "asset_dir = os.path.join(\"api\", asset_id)\n",
    "os.makedirs(asset_dir, exist_ok=True)\n",
    "\n",
    "# Load data for this asset\n",
    "hp_list = []\n",
    "restriction_list = []\n",
    "\n",
    "for chunk in pd.read_csv(DATA_PATH, \n",
    "                         usecols=[\"asset\", \"HydraulicHorsepower\", \"AirFilterRestriction\"], \n",
    "                         chunksize=CHUNKSIZE):\n",
    "    mask = chunk[\"asset\"].astype(str) == asset_id\n",
    "    if mask.any():\n",
    "        hp_list.append(chunk.loc[mask, \"HydraulicHorsepower\"].values)\n",
    "        restriction_list.append(chunk.loc[mask, \"AirFilterRestriction\"].values)\n",
    "\n",
    "hp = np.concatenate(hp_list)\n",
    "restriction = np.concatenate(restriction_list)\n",
    "\n",
    "max_hp_asset = asset_limits[asset_id]['max_hp']\n",
    "max_restriction_asset = asset_limits[asset_id]['max_restriction']\n",
    "\n",
    "print(f\"Loaded {len(hp):,} data points\")\n",
    "print(f\"HP range: {np.min(hp):.2f} to {np.max(hp):.2f}\")\n",
    "print(f\"Restriction range: {np.min(restriction):.2f} to {np.max(restriction):.2f}\")\n",
    "print(f\"Asset limits: HP={max_hp_asset}, Restriction={max_restriction_asset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09140e22",
   "metadata": {},
   "source": [
    "### 3.2 Bin Data and Compute Lower Envelope\n",
    "- Bin HP into fixed-width intervals\n",
    "- Compute 5th percentile restriction per bin (clean filter behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933bbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins\n",
    "hp_min, hp_max = np.min(hp), np.max(hp)\n",
    "bins = np.arange(hp_min, max(hp_max, max_hp_asset) + HP_BIN_WIDTH, HP_BIN_WIDTH)\n",
    "bin_indices = np.digitize(hp, bins) - 1\n",
    "\n",
    "# Compute 5th percentile per bin\n",
    "bin_centers = []\n",
    "bin_restriction = []\n",
    "\n",
    "for i in range(len(bins) - 1):\n",
    "    mask = bin_indices == i\n",
    "    bin_center = (bins[i] + bins[i+1]) / 2\n",
    "    if np.any(mask):\n",
    "        perc = np.percentile(restriction[mask], PERCENTILE)\n",
    "        bin_centers.append(bin_center)\n",
    "        bin_restriction.append(perc)\n",
    "    else:\n",
    "        bin_centers.append(bin_center)\n",
    "        bin_restriction.append(np.nan)\n",
    "\n",
    "bin_centers = np.array(bin_centers)\n",
    "bin_restriction = np.array(bin_restriction)\n",
    "\n",
    "# Remove empty bins\n",
    "fit_mask = ~np.isnan(bin_restriction)\n",
    "fit_bin_centers = bin_centers[fit_mask]\n",
    "fit_bin_restriction = bin_restriction[fit_mask]\n",
    "\n",
    "print(f\"Created {len(fit_bin_centers)} bins with data\")\n",
    "print(f\"HP bin range: {fit_bin_centers.min():.2f} to {fit_bin_centers.max():.2f}\")\n",
    "print(f\"Restriction range (5th %ile): {fit_bin_restriction.min():.2f} to {fit_bin_restriction.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de322ec6",
   "metadata": {},
   "source": [
    "### 3.3 Fit Isotonic Regression\n",
    "Ensures monotonic relationship: restriction never decreases as HP increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting Isotonic Regression...\")\n",
    "iso = IsotonicRegression(increasing=True, out_of_bounds=\"clip\")\n",
    "iso.fit(fit_bin_centers, fit_bin_restriction)\n",
    "\n",
    "# Get predictions\n",
    "iso_pred = iso.predict(fit_bin_centers)\n",
    "\n",
    "print(f\"Model fitted:\")\n",
    "print(f\"  Input HP range: {fit_bin_centers.min():.2f} to {fit_bin_centers.max():.2f}\")\n",
    "print(f\"  Output Restriction range: {iso_pred.min():.2f} to {iso_pred.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbba947",
   "metadata": {},
   "source": [
    "### 3.4 Compute Initial Linear Extrapolation\n",
    "Use the last rising segment to extrapolate beyond fitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the last segment with significant positive slope\n",
    "slopes = np.diff(iso_pred) / np.diff(fit_bin_centers)\n",
    "SLOPE_THRESHOLD = 0.001\n",
    "still_rising = slopes > SLOPE_THRESHOLD\n",
    "\n",
    "if np.any(still_rising):\n",
    "    N_POINTS_FOR_EXTRAP = 10\n",
    "    last_rising_indices = np.where(still_rising)[0]\n",
    "    if len(last_rising_indices) >= N_POINTS_FOR_EXTRAP:\n",
    "        extrap_indices = last_rising_indices[-N_POINTS_FOR_EXTRAP:]\n",
    "    else:\n",
    "        extrap_indices = last_rising_indices\n",
    "    X_extrap = fit_bin_centers[extrap_indices]\n",
    "    y_extrap = iso_pred[extrap_indices]\n",
    "    extrapolation_slope, extrapolation_intercept = np.polyfit(X_extrap, y_extrap, 1)\n",
    "else:\n",
    "    print(\"Warning: No rising trend, using last two points\")\n",
    "    X_extrap = fit_bin_centers[-2:]\n",
    "    y_extrap = iso_pred[-2:]\n",
    "    extrapolation_slope, extrapolation_intercept = np.polyfit(X_extrap, y_extrap, 1)\n",
    "\n",
    "print(f\"Initial extrapolation from last {len(X_extrap)} points\")\n",
    "print(f\"  Slope: {extrapolation_slope:.6f} Restriction per HP\")\n",
    "print(f\"  Intercept: {extrapolation_intercept:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ddec0",
   "metadata": {},
   "source": [
    "### 3.5 Force Extrapolation to Asset Limits\n",
    "**Critical Step**: Adjust slope to ensure we reach max_restriction at max_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47be2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_at_extrap_start = fit_bin_centers.max()\n",
    "r_at_extrap_start = iso.predict([[hp_at_extrap_start]])[0]\n",
    "restriction_at_max_hp_natural = extrapolation_slope * max_hp_asset + extrapolation_intercept\n",
    "\n",
    "print(f\"Natural extrapolation reaches R={restriction_at_max_hp_natural:.2f} at HP={max_hp_asset}\")\n",
    "print(f\"Asset limit requires: R={max_restriction_asset} at HP={max_hp_asset}\")\n",
    "\n",
    "if restriction_at_max_hp_natural < max_restriction_asset:\n",
    "    print(f\"\\n⚠️ FORCING STEEPER SLOPE to reach asset limits\")\n",
    "    extrapolation_slope = (max_restriction_asset - r_at_extrap_start) / (max_hp_asset - hp_at_extrap_start)\n",
    "    extrapolation_intercept = r_at_extrap_start - extrapolation_slope * hp_at_extrap_start\n",
    "    print(f\"  Adjusted slope: {extrapolation_slope:.6f}\")\n",
    "    print(f\"  Now reaches R={max_restriction_asset} at HP={max_hp_asset}\")\n",
    "    hp_at_max_restriction = max_hp_asset\n",
    "else:\n",
    "    print(\"✓ Natural extrapolation is sufficient\")\n",
    "    if extrapolation_slope > 0:\n",
    "        hp_at_max_restriction = (max_restriction_asset - extrapolation_intercept) / extrapolation_slope\n",
    "    else:\n",
    "        hp_at_max_restriction = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd6a99",
   "metadata": {},
   "source": [
    "### 3.6 Define Prediction Function\n",
    "Combines isotonic model with linear extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_restriction_from_hp(hp_values, iso_model, max_fitted_hp, \n",
    "                                slope, intercept, max_restriction_limit):\n",
    "    \"\"\"Predict restriction from HP using hybrid isotonic + linear model\"\"\"\n",
    "    hp_values = np.atleast_1d(hp_values)\n",
    "    predictions = np.zeros_like(hp_values, dtype=float)\n",
    "    \n",
    "    for i, hp_val in enumerate(hp_values):\n",
    "        if hp_val <= max_fitted_hp:\n",
    "            # Use isotonic model for fitted range\n",
    "            predictions[i] = iso_model.predict([[hp_val]])[0]\n",
    "        else:\n",
    "            # Use linear extrapolation beyond fitted range\n",
    "            predictions[i] = slope * hp_val + intercept\n",
    "        # Cap at asset limit\n",
    "        predictions[i] = min(predictions[i], max_restriction_limit)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d826b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package model with all parameters\n",
    "model_data = {\n",
    "    'iso': iso,\n",
    "    'max_hp_fitted': fit_bin_centers.max(),\n",
    "    'min_hp_fitted': fit_bin_centers.min(),\n",
    "    'extrap_slope': extrapolation_slope,\n",
    "    'extrap_intercept': extrapolation_intercept,\n",
    "    'max_restriction_fitted': iso_pred.max(),\n",
    "    'min_restriction_fitted': iso_pred.min(),\n",
    "    'max_hp_asset': max_hp_asset,\n",
    "    'max_restriction_asset': max_restriction_asset,\n",
    "    'hp_at_max_restriction': hp_at_max_restriction\n",
    "}\n",
    "\n",
    "model_path = os.path.join(asset_dir, \"model.pkl\")\n",
    "joblib.dump(model_data, model_path)\n",
    "print(f\"✓ Model saved to {model_path}\")\n",
    "\n",
    "# Save bin summary\n",
    "bins_csv_path = os.path.join(asset_dir, f\"{asset_id}_model_bins.csv\")\n",
    "with open(bins_csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"hp_bin_center\", \"restriction_5th_percentile\"])\n",
    "    for center, val in zip(bin_centers, bin_restriction):\n",
    "        writer.writerow([center, val])\n",
    "print(f\"✓ Bin data saved to {bins_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cdfc2d",
   "metadata": {},
   "source": [
    "### 3.7 Visual Validation\n",
    "Plot shows:\n",
    "- Raw data (light blue scatter)\n",
    "- 5th percentile bins (green dots)\n",
    "- Isotonic fit (red solid)\n",
    "- Linear extrapolation (red dashed)\n",
    "- Asset limits (vertical/horizontal lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Raw data\n",
    "plt.scatter(hp, restriction, s=1, alpha=0.1, label=\"All Data\", color='lightblue')\n",
    "\n",
    "# Binned 5th percentile\n",
    "plt.plot(fit_bin_centers, fit_bin_restriction, 'go', markersize=6, \n",
    "         label=\"5th Percentile per Bin\", zorder=5)\n",
    "\n",
    "# Isotonic fit\n",
    "hp_plot = np.linspace(fit_bin_centers.min(), fit_bin_centers.max(), 500)\n",
    "restriction_plot = iso.predict(hp_plot)\n",
    "plt.plot(hp_plot, restriction_plot, 'r-', linewidth=2, \n",
    "         label=\"Isotonic Baseline (fitted)\", zorder=4)\n",
    "\n",
    "# Linear extrapolation\n",
    "hp_extrap = np.linspace(fit_bin_centers.max(), max_hp_asset, 100)\n",
    "restriction_extrap = extrapolation_slope * hp_extrap + extrapolation_intercept\n",
    "restriction_extrap = np.minimum(restriction_extrap, max_restriction_asset)\n",
    "plt.plot(hp_extrap, restriction_extrap, 'r--', linewidth=2, \n",
    "         label=\"Linear Extrapolation (forced)\", zorder=4)\n",
    "\n",
    "# Reference lines\n",
    "plt.axvline(fit_bin_centers.max(), color='orange', linestyle=':', linewidth=1.5,\n",
    "            label=f\"Extrapolation starts at HP={fit_bin_centers.max():.0f}\", zorder=3)\n",
    "plt.axhline(max_restriction_asset, color='purple', linestyle='-.', linewidth=1.5, alpha=0.5,\n",
    "            label=f\"Asset Restriction Limit ({max_restriction_asset})\", zorder=3)\n",
    "plt.axvline(max_hp_asset, color='brown', linestyle='-.', linewidth=1.5, alpha=0.5,\n",
    "            label=f\"Asset HP Limit ({max_hp_asset})\", zorder=3)\n",
    "\n",
    "if hp_at_max_restriction <= max_hp_asset:\n",
    "    plt.plot(hp_at_max_restriction, max_restriction_asset, 'rs', markersize=10,\n",
    "             label=f\"Reaches Max R at HP={hp_at_max_restriction:.0f}\", zorder=6)\n",
    "\n",
    "plt.xlabel(\"HydraulicHorsepower\", fontsize=12)\n",
    "plt.ylabel(\"AirFilterRestriction\", fontsize=12)\n",
    "plt.title(f\"Clean Baseline Curve Fit: R_clean(HP) - Asset {asset_id}\", fontsize=14)\n",
    "plt.legend(loc='best', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, max_hp_asset * 1.05)\n",
    "plt.ylim(0, max_restriction_asset * 1.1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = os.path.join(asset_dir, f\"{asset_id}_model_baseline.png\")\n",
    "plt.savefig(plot_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"✓ Plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6900648",
   "metadata": {},
   "source": [
    "### 3.8 Test Prediction Function\n",
    "Verify model predictions across full HP range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hps = [200, 500, 1000, 1200, 1500, 1800, 2000, int(max_hp_asset)]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing R_clean(HP) predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for hp_test in test_hps:\n",
    "    r_pred = predict_restriction_from_hp(\n",
    "        hp_test, iso, \n",
    "        fit_bin_centers.max(),\n",
    "        extrapolation_slope, \n",
    "        extrapolation_intercept,\n",
    "        max_restriction_asset\n",
    "    )\n",
    "    in_range = \"✓ fitted\" if hp_test <= fit_bin_centers.max() else \"✗ extrapolated\"\n",
    "    print(f\"HP={hp_test:4.0f} → R_clean={r_pred[0]:6.2f}  {in_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e526c",
   "metadata": {},
   "source": [
    "## Step 4: Process All Assets\n",
    "Wrap the above logic in a loop to process all assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For production, wrap cells 6-14 in a loop:\n",
    "for asset_id in asset_ids:\n",
    "    print(f\"\\n{'='*60}\\nProcessing asset {asset_id}\\n{'='*60}\")\n",
    "    # ... (all the processing steps from cells 6-14)\n",
    "    print(f\"✓ Asset {asset_id} complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322e3f5",
   "metadata": {},
   "source": [
    "## Step 5: Create Combined Model File\n",
    "Package all asset models into single file for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa33567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_models(asset_ids, base_dir=\"api\"):\n",
    "    \"\"\"Combine all asset models into single file\"\"\"\n",
    "    models = {}\n",
    "    for asset_id in asset_ids:\n",
    "        model_path = os.path.join(base_dir, asset_id, \"model.pkl\")\n",
    "        if os.path.exists(model_path):\n",
    "            models[asset_id] = joblib.load(model_path)\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: {model_path} not found, skipping.\")\n",
    "    \n",
    "    output_path = os.path.join(base_dir, \"all_models.pkl\")\n",
    "    joblib.dump(models, output_path)\n",
    "    print(f\"✓ Combined model saved to {output_path}\")\n",
    "    print(f\"✓ Total assets: {len(models)}\")\n",
    "    return models\n",
    "\n",
    "all_models = combine_all_models(asset_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15ad0c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "1. **Lower envelope extraction**: 5th percentile binning captures clean filter behavior\n",
    "2. **Monotonic fitting**: Isotonic regression respects physical constraints\n",
    "3. **Forced extrapolation**: Linear extension ensures full range coverage\n",
    "4. **Asset-specific models**: Customized to each machine's operating envelope\n",
    "\n",
    "### Key Assumptions\n",
    "- ⚠️ Linear relationship holds in unobserved high-HP range\n",
    "- ⚠️ 5th percentile adequately represents \"clean\" baseline\n",
    "- ⚠️ Historical data contains sufficient clean filter examples\n",
    "\n",
    "### Next Steps\n",
    "- **Validation**: Compare predictions against known clean filter operation\n",
    "- **Anomaly detection**: Use R_clean(HP) as baseline to flag degraded filters\n",
    "- **Monitoring**: Track when actual restriction exceeds baseline by threshold\n",
    "- **Refinement**: Update models as more clean baseline data becomes available\n",
    "\n",
    "### Model Outputs\n",
    "- `api/{asset}/model.pkl` - Serialized model + parameters\n",
    "- `api/{asset}/{asset}_model_bins.csv` - Binned training data\n",
    "- `api/{asset}/{asset}_model_baseline.png` - Visual validation\n",
    "- `api/all_models.pkl` - Combined deployment artifact"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
